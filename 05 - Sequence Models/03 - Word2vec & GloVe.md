# Learning Word Embeddings
 
 - Word embedding algorithms evolved from complex to simpler methods, achieving similar results, especially with large datasets.
 - A neural network predicts the next word in a sequence by transforming words into dense embedding vectors using a parameter matrix $ E $.
 - Embeddings are fed into the network, followed by a softmax layer for prediction.
 - A fixed number of previous words (e.g., 4) is used to predict the next word, ensuring consistent input size.
 - The model learns similar embeddings for related words (e.g., "orange" and "apple") to improve predictions.
 - Alternatives like the Skip-Gram model use simpler contexts (e.g., one word) to predict target words, effectively learning word embeddings.
 - While complex models can build language models, simpler models like Word2Vec can also learn effective word embeddings.

# Word2Vec

 - Word2Vec is an algorithm designed to learn word embeddings from a corpus of text.
 - It operates using either the: 
    - **Skip-Gram model**, which predicts surrounding words given a target word. 
    - **Continuous Bag of Words (CBOW) model**, which predicts a target word based on surrounding context words.
 - By training on large text corpora, Word2Vec captures semantic relationships between words, producing dense vector representations where words with similar meanings are located closer together in the vector space.
 - It's downside is the computational cost of evaluating the softmax function, especially with large vocabularies.

**Skip-Gram Model Overview**

 - The Skip-Gram model is used to create a supervised learning problem by predicting target words based on a given context word.
 - Unlike fixed context windows, the model randomly selects a context word $ c $ and a target word $ t $ within a certain range.
 - For example, give the phrase "I want a glass of orange juice to go along with my cereal".
 - If "orange" is chosen as the context word, possible target words could be "juice," "glass," or "my."
 - The goal is not to excel in the supervised task itself but to learn good word embeddings from the process.

**Model Architecture**

 - The input word is represented as a one-hot vector $ \mathbf{O}_c $ for the context word.
 - The embedding matrix $ E $ is used to convert the one-hot vector into an embedding vector $ \mathbf{e}_c = E \times \mathbf{O}_c $.
 - The embedding vector $ \mathbf{e}_c $ is fed into a softmax unit to predict the probability distribution over the target words.
 - The probability of a target word $ t $ given the context word $ c $ is calculated using the softmax function:
    $$ P(t|c) = \frac{e^{\theta_t^\top \mathbf{e}_c}}{\sum_{j=1}^{V} e^{\theta_j^\top \mathbf{e}_c}} $$
    Where:
    - $ V $ is the size of the vocabulary.
    - $ t $ is the target word.
    - $ c $ is the context word.
    - $ \theta_t^\top $ is the transpose of the parameter vector associated with the target word $ t $.
    - $ \mathbf{e}_c $ is the embedding vector for the context word $ c $.
    
**Loss Function**

 - The loss function used is the negative log-likelihood for a single example:
    $$ L = -\sum_{i=1}^{V} y_i \log \hat{y}_i $$
    Where:
    - $ V $ is the vocabulary size.
    - $ y_i $ is the true one-hot encoded vector for the target word.
    - $ \hat{y}_i $ is the predicted probability distribution over the vocabulary.

# Negative Sampling

 - Negative Sampling is a modification of the Skip-Gram model.
 - It enables a more efficient learning algorithm by creating a new supervised learning problem.
 - The task is to predict whether a pair of words (context and target) appear together or are randomly paired.
 - Negative Sampling reduces the computational cost by transforming the problem into $ V $ binary logistic regression problems.

**Creating the Dataset**

 - The model is trained on a generated dataset, where:
    - The positive examples are generated by selecting a context word $ c $ and a target word $ t $.
    - The negative examples are created by pairing the same context word $ c $ with random words from the vocabulary.
 - Typically, for each positive example, $ k $ negative examples are generated:
    - For smaller datasets, $ k $ can be larger (e.g., 5-20).
    - For larger datasets, $ k $ is typically smaller (e.g., 2-5).

**Training Process**

 - The model is trained using logistic regression.
 - For each context-target pair $(c, t)$, the model predicts whether $ y = 1 $ (positive) or $ y = 0 $ (negative).
 - The training set is formed by multiple such pairs, with $ k $ negative examples for each positive one.
 - The logistic regression model computes the probability $ P(y=1 | c, t) $ using:
    $$ P(y=1 | c, t) = \sigma(\theta_t^\top e_c) $$
    Where:
    - $ \theta_t $: Parameter vector for the target word.
    - $ e_c $: Embedding vector for the context word.
    - $ \sigma $: Sigmoid function.
 - The model is trained by updating only the parameters corresponding to the positive and $ k $ negative examples per iteration.

# GloVe Word Vectors

 - GloVe stands for Global Vectors for Word Representation.
 - Created by Jeffrey Pennington, Richard Socher, and Chris Manning.
 - GloVe is popular for its simplicity and is an alternative to the Word2Vec and Skip-Gram models.

**Optimization Objective**

 - The goal of the GloVe algorithm is to minimize the following loss function:
    $$ \text{Minimize} \quad \sum_{i=1}^{V} \sum_{j=1}^{V} f(X_{ij}) \left( \theta_i^T e_j + b_i + b_j' - \log(X_{ij}) \right)^2 $$
    Where:
   - $ f(X_{ij}) $: determines how much each word pair $i, j$ contributes to the overall loss.
   - $ \theta_i^T e_j $: measures the similarity between the word $i$ and the word $j$.
   - $ b_i $ and $ b_j' $: help adjust the overall similarity score.
   - $ \log(X_{ij})$: serves as the target value that the model tries to approximate using the dot product and bias terms.
 - By minimizing this difference across all word pairs in the corpus, the model learns embeddings that capture meaningful relationships between words.
