# Regularization

 - Overfitting occurs when a neural network has high variance, meaning it performs well on training data but poorly on new, unseen data.
 - One way to address high variance is by obtaining more training data, though this can be expensive or impractical.
 - Regularization is a more accessible solution to reduce variance and prevent overfitting in your model.
 - It discourages the model from learning overly complex patterns that might only fit the training data, thus helping it generalize better to new data.

**Regularization in Neural Networks**

 - In neural networks, the cost is a function of all parameters $ w^{[1]}, b^{[1]}, \dots, w^{[L]}, b^{[L]} $, where $ L $ is the number of layers.
 - Regularization is added by incorporating the term $ \frac{\lambda}{2m} \sum_{l=1}^{L} \|w^{[l]}\|_F^2 $ into the cost function.
 - The Frobenius norm $ \|w^{[l]}\|_F^2 $ of a matrix $ w^{[l]} $ is defined as the sum of the squares of all elements in the matrix:
    $$ \|w^{[l]}\|_F^2 = \sum_{i=1}^{n[l]} \sum_{j=1}^{n[l-1]} (w_{ij}^{[l]})^2 $$
 - The regularized cost function with regularization becomes:
    $$ J_{\text{reg}}(\theta) = \frac{1}{m} \sum_{i=1}^{m} \mathcal{L}(y^{(i)}, \hat{y}^{(i)}) + \frac{\lambda}{2m} \sum_{l=1}^{L} \|w^{[l]}\|_F^2 $$

**Gradient Descent with Regularization**

 - The gradient of the cost function with respect to $ w $ is updated to include the regularization term:
      $$ dW = \frac{\partial J}{\partial w} = \frac{\partial J_{\text{original}}}{\partial w} + \frac{\lambda}{m} w  $$
 - The weight update rule becomes:
      $$ w^{[l]} \gets w^{[l]} - \alpha \left(\frac{\partial J}{\partial w^{[l]}} + \frac{\lambda}{m} w^{[l]}\right) $$
 - This process gradually decays the weights by a factor slightly less than 1, hence the term "weight decay."

**Choosing the Regularization Parameter $ \lambda $**

 - $ \lambda $ is a hyperparameter that must be tuned using a development set or cross-validation.
 - It controls the trade-off between fitting the training data well and keeping the model weights small to prevent overfitting.
 - In practice, you may need to experiment with different values of $ \lambda $ to find the optimal balance.

# Intuition Behind Regularization

 - As the regularization parameter $ \lambda $ increases, the weight matrices $ w^{[l]} $ are driven closer to zero.
 - By reducing the magnitude of the weights, regularization makes the network less capable of fitting complex, non-linear decision boundaries, which are often responsible for overfitting.
 - This results in a model that generalizes better to new data, reducing the variance problem.

Here is a detailed summary of the dropout regularization technique, including key points and relevant code snippets:

# Dropout Regularization

 - Dropout is a regularization technique used to prevent overfitting in neural networks.
 - It works by randomly "dropping out" (setting to zero) a subset of neurons during training.
 - For each layer, a probability is assigned (e.g., 0.5 or 0.8) to decide whether to keep or drop each neuron.
 - The network trains on a reduced version of itself with some neurons removed, and this process varies for each training example.
 - This prevents the network from becoming overly reliant on any particular neuron or subset of neurons, leading to a more generalized model.

**Inverted Dropout**

 - Inverted Dropout ensures that the expected value of the activations remains consistent during training and testing by scaling the activations during training.
 - Bellow is a Python code implementing the inverted dropout for layer l=3.
  
    ```python
    import numpy as np
    
    # Probability of keeping a neuron
    keep_prob = 0.8
    
    # Generate dropout mask
    d3 = np.random.rand(a3.shape[0], a3.shape[1]) < keep_prob
    
    # Apply dropout mask to activations
    a3 = np.multiply(a3, d3)
    
    # Scale activations to maintain expected value
    a3 /= keep_prob
    ```
 
 - `d3` is a mask generated by comparing random values to `keep_prob`.
 - If `keep_prob` is 0.8, 80% of the neurons are kept (`True` in the mask) and 20% are dropped (`False` in the mask).
 - The element-wise multiplication `a3 = np.multiply(a3, d3)` zeroes out the activations where `d3` is `False`.
 - The scaling `a3 /= keep_prob` ensures that the expected value of the activations remains unchanged.

**No Dropout During Testing**

 - During testing, dropout is not applied, meaning all neurons are active.
 - The network uses the full set of learned weights, which have been regularized during training.
 - Inverted Dropout ensures that the scaling done during training makes the activations consistent, so no additional scaling is required during testing.

# Early Stopping

 - Prevent overfitting by halting the training process before the model starts to overfit.
 - Step-by-step on how to perform early stop:
    1. Monitor the training error and the development (dev) set error during training.
        - Training error typically decreases monotonically as training progresses.
        - Dev set error decreases initially but begins to increase when overfitting occurs.
    2. Stop training when the dev set error starts to increase, and take the parameters at that point as the final model.
 - Early stopping limits the growth of the model parameters (e.g., weights $ w $), keeping them smaller, which reduces overfitting.
 - Nevetherless, it mixes the task of optimizing the cost function $ J $ with the task of regularizing the model.
 - Ideally, these tasks should be treated independently to simplify the optimization process.